# -*- coding: utf-8 -*-
"""StockPricePredictionProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TlOJPk9NzMGN4dqKpXCSZSIlIpBTKqKe
"""

# Description: This program uses an artificial recurrent neural network called Long Short Term Memory (LSTM) to predict the closing stock price of a corporation (Apple Inc.) using the past 60 day stock price.
# link: https://www.youtube.com/watch?v=QIUxPv5PJOY&t=2398s

#Import the libraries
import math
import pandas_datareader as web
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

#Get the stock quote
df = web.DataReader('AAPL', data_source='yahoo', start='2012-01-01', end='2019-12-17')
df

#GEt the number of rows and columns in the data set
df.shape

#Visualise the closing price history
plt.figure(figsize=(16,8))
plt.title('Close Price History')
plt.plot(df['Close'])
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price USD ($)', fontsize=18)
plt.show()

#Create a new dataframe with only the 'Close' column
data = df.filter(['Close'])
#Convert the dataframe to a numpy array
dataset = data.values
#Get the number of rows to train the model on
training_data_len = math.ceil( len(dataset) * .8)

training_data_len

#Scale the data (it is nearly always advantageous to apply pre-processing transformations, scaling/normalisation to the input data before it is presented to a neural network (good practice) it helps the model)
scaler = MinMaxScaler(feature_range=(0,1))
#transform data to be between 0-1, scaled_data will hold this dataset
scaled_data = scaler.fit_transform(dataset)

#scaler.fit_transform computes the minimum and maximum to be used for scaling and then transforms the data based on these two values. Range will be 0-1 inclusive, could be 1 or 0 or anything in between

scaled_data

#Create the training data set
#Create the scaled training data set it will contain all of the values from index 0 to the training data length and then we want to get back all of the columns 
train_data = scaled_data[0:training_data_len, :]
#Split the data in to x_train and y_train data sets
#x_train will be the independant training variables/features. y_train will be the dependant/target variables
x_train = []
y_train =[]

#append the last 60 values to our x_train dataset. 
#For the first past through x_train will contain 60 values, and those values will be indexed from position 0 to position 59.
#For the first pass through y_train will contain the 61st value, that we want our model to predict, which will be at position 60
for i in range(60, len(train_data)):
      x_train.append(train_data[i-60:i,0])
      y_train.append(train_data[i,0])
      if i<= 61:
        print(x_train)
        print(y_train)
        print()

#Convert the x_train and the y_train dataset to numpy arrays so we can use them for training the LSTM model
x_train, y_train = np.array(x_train), np.array(y_train)

#Reshape the data. A LSTM network expects the input to be 3D in the form of number of samples, number of timestamps and number of features. Right now training data is 2D
x_train = np.reshape(x_train,(x_train.shape[0], x_train.shape[1], 1))
x_train.shape

#Build the LSTM Model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))

#Compile the model. An optimiser is useed to improve upon the loss function. Loss function is used to measure how well the model did on training
model.compile(optimizer='adam', loss='mean_squared_error')

#Train the model - epoch: number of iterations when an entire data set is passed forward and back through a neural network
model.fit(x_train, y_train, batch_size=1, epochs=1)

#Create the testing dataset
#Create a new array containing scaled values from index 1543 to 2003 (the scaled testing dataset)
test_data = scaled_data[training_data_len - 60:, :]
#Create the data sets x_test and y_test
x_test = []

#from index training_data_length to the rest of the data, and get back all the columns as well. 
#y_test will be all the values that we want our model to predict. 
y_test = dataset[training_data_len:, :]

#Create x_test set. x_test dataset will contain the past 60 values and y_test contains the actual values, the 61st values, but it is not scaled. these values are the normal values
for i in range(60, len(test_data)):
  x_test.append(test_data[i-60:i, 0])

#Convert the data to a numpy array (so that we can use it in the LSTM model)
x_test = np.array(x_test)
x_test

#REshape the data (because our dataset is 2D and LSTM is expecting a 3D shape)
# np.reshape(x_test, (number of rows, the number of columns which is equal to the number of time steps, number of features (which is the Close Price))

x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))
x_test

#Get the models predicted price values(for the x_test dataset)
#we want it to be the exacr same values that are in out y_test dataset once we inverse transform this data
predictions = model.predict(x_test)

#inverse transform the data (unscaling the values) 
#we want predictions to contain the same values as y_test 

#predictions based off x_test dataset
predictions = scaler.inverse_transform(predictions)

#Get the root mean squared error (RMSE) a measure of how accurate the model predicts the response. It is the standard deviation of the residuals. 
#The lower values of RMSE indicate a better fit. evaluate model with other metrics to get an idea of how well your model performs
rmse = np.sqrt(np.mean( predictions - y_test )**2 )
rmse

#Plot the data
train = data[:training_data_len]
valid = data[training_data_len:]
valid['Predictions'] = predictions
#Visualise the data
plt.figure(figsize=(16,8))
plt.title('Model')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price USD ($)', fontsize=18)
plt.plot(train['Close'])
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train','Val','Predictions'], loc='lower right')
plt.show

#Show the vaild price and the predicted prices
valid

#Get the quote
apple_quote = web.DataReader('AAPL', data_source='yahoo', start='2012-01-01', end='2020-06-11')

#Create a new data frame
new_df = apple_quote.filter(['Close'])
#Get the last 60 day closing price values and convert the dataframe to an array
last_60_days = new_df[-60:].values
#Scale the data to be values between 0 and 1 (using a variable from before called scaler, not using fit transform because we want to transform the data using same min and max values that we used before )
last_60_days_scaled = scaler.transform(last_60_days)
#Create an empty list
X_test = []
#Append past 60 days to X_test
X_test.append(last_60_days_scaled)
#Convert the x_test datsset to a np array
X_test = np.array(X_test)
#Reshape the data
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
#Get tje predicted scaled price
pred_price = model.predict(X_test)
#Undo the scaling
pred_price = scaler.inverse_transform(pred_price)
print(pred_price)

#Get the quote
apple_quote2 = web.DataReader('AAPL', data_source='yahoo', start='2020-06-12', end='2020-06-12')
print(apple_quote2['Close'])